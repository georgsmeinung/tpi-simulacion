[
  {
    "objectID": "tpi-simulation-in-data-science.html#simulación-y-optimización-en-ciencia-de-datos",
    "href": "tpi-simulation-in-data-science.html#simulación-y-optimización-en-ciencia-de-datos",
    "title": "Maestría en Ciencia de Datos 2024/2025",
    "section": "Simulación y Optimización en Ciencia de Datos",
    "text": "Simulación y Optimización en Ciencia de Datos"
  },
  {
    "objectID": "tpi-simulation-in-data-science.html#trabajo-práctico-integral",
    "href": "tpi-simulation-in-data-science.html#trabajo-práctico-integral",
    "title": "Maestría en Ciencia de Datos 2024/2025",
    "section": "Trabajo Práctico Integral",
    "text": "Trabajo Práctico Integral\nProfesores:\n\nDEL ROSSO, Rodrigo\nNUSKE, Ezequiel\n\nIntegrantes:\n\nCANCELAS, Martín\nFILIPUZZI, Juan Manuel\nNICOLAU, Jorge\n\n\nIntroducción\nEste Trabajo Práctico (TP) integra los contenidos vistos en las clases\n\nUnidad I – Generación de números pseudoaleatorios y Monte Carlo\nUnidad II – Bayes, cadenas de Markov y Metropolis–Hastings\nUnidad III – Simulación de eventos discretos (SED)\nUnidad IV – Procesos continuos (NHPP, CTMC, SDE)\nUnidad V – Reacciones químicas estocásticas: Gillespie SSA y Next Reaction Method\n\nEl objetivo es que el alumno implemente técnicas de simulación, compare métodos, valide sus resultados y presente visualizaciones claras.\n\n\nSobre el código fuente de este trabajo\nPara facilitar la lectura y comprensión del trabajo, el código fuente completo de los graficos, tablas se encuentran disponibles en el siguiente repositorio de GitHub: https://github.com/georgsmeinung/tpi-simulacion/ en el Notebook Jupyter tpi-simulation-in-data-science.ipynb\nMientras que el el código base (originalmente en R, transformado a Python) se puede encontrar en el Anexo I de este documento.\n\n\nConfiguración\nPara el presente trabajo se utilizan los siguientes parámetros globales y semillas, además los parámetros específicos del renderizado de gráficos. Para claridad estos últimos se encuentran en el Anexo de Código Fuente de Gráficos.\n\n# CONFIGURACIÓN GENERAL DE LAS SIMULACIONES\n# Parámetros (LCG estándar de C++ minstd_rand)\nlgc_a, lgc_c, lgc_m = 48271, 0, 2**31 - 1\n# Semilla\nrnd_seed = 2371\n\nAsimismo, se utilizan las siguientes librerías\n\nimport arviz as az                                               \nimport heapq                                                     \nimport matplotlib.animation as animation                         \nimport matplotlib.font_manager as fm                             \nimport matplotlib.pyplot as plt                                  \nimport numpy as np                                               \nimport random                                                    \nimport scipy.stats as stats\n\nfrom IPython.display import Image, display, Markdown                       \nfrom matplotlib.collections import LineCollection, PolyCollection\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\n\nParte 1 - Generación de Números Pseudoaleatorios y Monte Carlo\n\nImplementación de un Generador Congruencial Lineal (LCG)\nLa implementación de un Generador Congruencial Lineal (LCG) se fundamenta en un algoritmo iterativo y determinista regido por la relación de recurrencia\n\\[X_{n+1} = (aX_n + c) \\mod m\\]\nmediante la cual se produce una secuencia de números pseudoaleatorios a partir de un valor inicial denominado semilla (\\(X_0\\)). El proceso requiere la definición de tres constantes enteras —el multiplicador (\\(a\\)), el incremento (\\(c\\)) y el módulo (\\(m\\))— y opera calculando el siguiente valor de la serie al multiplicar el estado actual por \\(a\\), sumar \\(c\\) y obtener el residuo de la división por \\(m\\), resultando en una sucesión periódica que, si bien carece de aleatoriedad verdadera, es computacionalmente eficiente y totalmente reproducible si se mantienen los mismos parámetros iniciales.\nAl usar operaciones aritméticas básicas, es extremadamente rápido computacionalmente. Como hay un número finito de resultados posibles (de \\(0\\) a \\(m-1\\)), la secuencia eventualmente se repetirá. A esto se le llama el “periodo”. Por otra parte si se usa la misma semilla (\\(X_0\\)), se obtendrá exactamente la misma secuencia de números. Esto es útil para “debugging” o reproducir simulaciones científicas, pero malo para la seguridad. No es seguro criptográficamente: No se debe usar para contraseñas o claves de seguridad, ya que es relativamente fácil predecir los siguientes números analizando la secuencia previa.\n\n# Implementación del LCG\ndef lcg_generator(seed, a, c, m, n):\n    numbers = []\n    x = seed\n    for _ in range(n):\n        x = (a * x + c) % m\n        # Normalizar a [0, 1]\n        numbers.append(x / m) \n\n    return numbers\n\nCon este algoritmo generamos una secuencia de números pseudoaleatorios uniformemente distribuidos en el intervalo [0, 1).\n\ndatos = lcg_generator(\n    seed=rnd_seed, a=lgc_a, c=lgc_c, m=lgc_m, n=5000\n)\n\n\n\n\n\nTabla 1 - Primeros 10 números generados\n\n\n\nÍndice\nValor\n\n\n\n\n01\n0.053295\n\n\n02\n0.611937\n\n\n03\n0.807027\n\n\n04\n0.010338\n\n\n05\n0.022969\n\n\n06\n0.754449\n\n\n07\n0.007679\n\n\n08\n0.677888\n\n\n09\n0.350518\n\n\n10\n0.868023\n\n\n\n\n\nPara evaluar la calidad de los números pseudoaleatorios generados por uel LCG, es necesario verificar dos propiedades fundamentales: Uniformidad e Independencia. A continuación, se detalla la lógica de implementación y el código en Python para las tres herramientas solicitadas.\n\nHistograma\nEl histograma visualiza la distribución de frecuencia de los números generados. Para un buen generador, esperamos una distribución uniforme (plana), lo que significa que cada intervalo del rango tiene aproximadamente la misma probabilidad de contener un número.\n\n\n\n\n\n\n\n\n\nLa presencia de alturas similares en todas las barras (“bins”) del histograma indica que el Generador Congruencial Lineal (LCG) está cumpliendo satisfactoriamente con la propiedad de uniformidad, lo que significa que cada sub-intervalo del rango tiene una probabilidad casi idéntica de contener un número generado. Esta distribución “plana” o rectangular sugiere que el algoritmo recorre el espacio muestral sin sesgos evidentes ni favoritismos hacia ciertos valores; sin embargo, es crucial notar que estas variaciones leves deben ser producto del azar natural (pequeñas fluctuaciones son esperadas y saludables), y aunque este patrón valida la equiprobabilidad, por sí solo no garantiza la independencia de los datos (ausencia de patrones secuenciales), por lo que un histograma visualmente equilibrado es una condición necesaria, pero no suficiente, para aprobar un generador.\n\n\nRuns Test (Prueba de Rachas/Corridas)\nEsta prueba verifica la independencia analizando la secuencia de oscilaciones. Una “racha” es una secuencia ininterrumpida de números crecientes o decrecientes.\nPara la lógica de Implementación (con Corridas Arriba/Abajo) se toma la secuencia \\(u_1, u_2, \\dots, u_n\\). Se crea una nueva secuencia binaria basada en si el valor actual es mayor o menor que el anterior (\\(+\\) si \\(u_i &lt; u_{i+1}\\), \\(-\\) si \\(u_i &gt; u_{i+1}\\)). Luego se cuenta el número total de rachas (cambios de \\(+\\) a \\(-\\) o viceversa). Se calcual el estadístico Z comparando el número de rachas obtenido con el esperado teóricamente usando la aproximación normal para muestras grandes (\\(N &gt; 20\\)).\n\\[\n\\mu_R = \\frac{2N - 1}{3}\n\\] \\[\n\\sigma_R^2 = \\frac{16N - 29}{90}\n\\] \\[\nZ = \\frac{R - \\mu_R}{\\sigma_R}\n\\]\nSi el valor absoluto de \\(Z\\) (\\(|Z|\\)) es mayor que el valor crítico (ej. \\(1.96\\) para un 95% de confianza), se rechaza la hipótesis de independencia.\n\n\n\n\n\n\n\n\n\n\n\n\nTabla 2 - Prueba de Rachas\n\n\n\nMétrica\nValor\n\n\n\n\nRachas Observadas\n3325.00\n\n\nRachas Esperadas (\\(\\mu\\))\n3333.00\n\n\nDesviación Std (\\(\\sigma\\))\n29.81\n\n\nZ-Score\n-0.2684\n\n\n\n\nRESULTADO: ALEATORIO (No se rechaza H0 al 95%)\n\n\nLa cercanía entre las rachas observadas y las esperadas revela una discrepancia numérica mínima, lo cual sugiere que el Generador Congruencial Lineal (LCG) cumple satisfactoriamente con la propiedad de independencia estadística. Este resultado indica que la secuencia de números fluctúa (asciende y desciende) con una frecuencia consistente con el azar puro, descartando la presencia de patrones de dependencia serial; al no existir una desviación significativa (la diferencia es muy pequeña considerando el tamaño de la muestra), el valor estadístico \\(Z\\) resultante sería muy cercano a cero, lo que impide rechazar la hipótesis nula y permite concluir que los datos no están correlacionados entre sí, validando el motor como un generador robusto en términos de oscilación.\n\nGráfico de Triples (Spectral Test Visual)\nEsta es una visualización en 3D para detectar correlaciones seriales. Los generadores LCG malos tienden a concentrar los puntos en planos discretos en lugar de llenar el espacio uniformemente (fenómeno conocido como planos de Marsaglia).\nEl gráfico de triples opera bajo la lógica de visualización del espacio de fases para detectar correlaciones seriales de largo alcance, mapeando tríadas consecutivas de la secuencia generada \\((u_n, u_{n+1}, u_{n+2})\\) como coordenadas espaciales \\((x, y, z)\\) en un cubo tridimensional. La premisa fundamental es que, si los números fueran verdaderamente independientes, los puntos deberían llenar el volumen de manera caótica y uniforme como una “nube de polvo”; sin embargo, debido a la naturaleza lineal de la fórmula \\(X_{n+1} = (aX_n + c)\\), los LCGs deficientes exhiben una estructura cristalina donde los puntos se alinean rígidamente en un número finito de planos paralelos (fenómeno conocido como planos de Marsaglia), revelando visualmente que la aleatoriedad es solo aparente y que existen dependencias matemáticas estrictas entre valores sucesivos.\n\n\n\n\n\n\n\n\n\nLa observación de una nube de puntos dispersa y volumétrica que llena el cubo de manera homogénea se interpreta como una validación exitosa de la calidad espectral del generador, indicando que la correlación serial entre ternas consecutivas \\((u_n, u_{n+1}, u_{n+2})\\) es despreciable o inexistente para fines prácticos. Visualmente, esto contrasta con los generadores deficientes que agrupan los puntos en “rebanadas” o planos paralelos separados; por el contrario, una distribución uniforme implica que el algoritmo posee un periodo lo suficientemente largo y unos parámetros adecuados para “romper” la estructura reticular visible, garantizando que no existen dependencias geométricas fuertes y haciendo al generador apto para simulaciones Monte Carlo multidimensionales.\n\n\nEstimación de \\(\\pi\\) por Monte Carlo\nLa estimación del valor de \\(\\pi\\) mediante el método de Monte Carlo utiliza el Generador Congruencial Lineal (LCG) para producir pares de coordenadas \\((x, y)\\) uniformemente distribuidas en el intervalo \\([0, 1)\\), simulando el lanzamiento aleatorio de puntos sobre un cuadrado unitario que contiene un cuarto de círculo inscrito. El procedimiento se fundamenta en la geometría probabilística: dado que el área del cuarto de círculo es \\(\\pi/4\\) y el área del cuadrado es \\(1\\), la probabilidad de que un punto caiga dentro del círculo es exactamente \\(\\pi/4\\); por consiguiente, al verificar cuántos puntos cumplen la condición \\(x^2 + y^2 \\le 1\\) y dividir esa cantidad por el número total de puntos generados, se obtiene una proporción que, multiplicada por \\(4\\), aproxima el valor de \\(\\pi\\), sirviendo esto a su vez como una prueba funcional de la calidad del LCG, ya que un generador sesgado o correlacionado (como el que forma líneas en el gráfico de triples) arrojará un valor de \\(\\pi\\) incorrecto al no cubrir el área uniformemente.\nLógica de Implementación 1. Generación de Pares: Se utiliza el LCG para obtener dos números consecutivos normalizados (\\(u_i, u_{i+1}\\)) que actúan como coordenadas \\(x\\) e \\(y\\). 2. Condición Geométrica: Se calcula la distancia al origen (\\(d = x^2 + y^2\\)). 3. Conteo: Si \\(d \\le 1\\), el punto está “dentro” del círculo (Acierto). 4. Cálculo Final: \\(\\pi \\approx 4 \\times \\frac{\\text{Aciertos}}{\\text{Total de Puntos}}\\).\nEn términos de Pyhton:\n\ndef simular_pi_montecarlo(n_puntos):\n    # Asumimos variables globales existen\n    s, a, c, m = rnd_seed, lgc_a, lgc_c, lgc_m\n\n    # Generamos el doble de puntos\n    lista_completa = lcg_generator(\n        s, a, c, m, n_puntos * 2\n    )\n    iterador_numeros = iter(lista_completa)\n\n    inside_x, inside_y = [], []\n    outside_x, outside_y = [], []\n    dentro_count = 0\n\n    for _ in range(n_puntos):\n        x = next(iterador_numeros)\n        y = next(iterador_numeros)\n\n        if x**2 + y**2 &lt;= 1.0:\n            dentro_count += 1\n            inside_x.append(x)\n            inside_y.append(y)\n        else:\n            outside_x.append(x)\n            outside_y.append(y)\n\n    pi_estimado = 4 * (dentro_count / n_puntos)\n\n    return (\n        pi_estimado,\n        inside_x, inside_y,\n        outside_x, outside_y\n    )\n\nAsí, utilizamos la implemntación de estimación de arriba en el siguiente código:\n\n# Cantidad de puntos para la estimación\nn = 10000\n\n# Ejecutar cálculo\nresultado = simular_pi_montecarlo(n)\n(\n    pi_est,\n    in_x, in_y,\n    out_x, out_y\n) = resultado\n\n# Calcular el error porcentual\nerror_pi = abs(np.pi - pi_est) / np.pi * 100\n\n\n\n\nTabla 3 - Resultado Estimación \\(\\pi\\)\n\n\n\nMétrica\nValor\n\n\n\n\nValor Final Estimado\n3.1496\n\n\nError Porcentual\n0.254882 %\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParte 2 - Metropolis–Hastings y Bayesian Inference\nLa relación entre el algoritmo de Metropolis-Hastings y la inferencia bayesiana es fundamentalmente instrumental, donde el primero actúa como la solución computacional para los desafíos analíticos planteados por el segundo. La inferencia bayesiana busca estimar la distribución posterior de parámetros desconocidos, denotada como \\(P(\\theta | D)\\), mediante la aplicación del Teorema de Bayes. No obstante, este proceso conlleva frecuentemente el cálculo de una constante de normalización —la evidencia marginal— que implica resolver integrales multidimensionales analíticamente intratables, lo cual impide la obtención directa de la distribución posterior en modelos complejos.\nEl algoritmo de Metropolis-Hastings, perteneciente a la familia de métodos de Monte Carlo vía Cadenas de Markov (MCMC), se distingue por su capacidad para generar muestras de una distribución de probabilidad objetivo sin necesidad de conocer su constante de normalización. El algoritmo opera construyendo una cadena de Markov que converge asintóticamente a la distribución deseada, requiriendo únicamente una función que sea proporcional a dicha densidad objetivo para evaluar los ratios de aceptación de las muestras propuestas.\nEn consecuencia, la conexión crítica reside en que la distribución posterior no normalizada en la estadística bayesiana es proporcional al producto de la función de verosimilitud y la distribución a priori (\\(Likelihood \\times Prior\\)). Dado que Metropolis-Hastings puede operar bajo condiciones de proporcionalidad, este algoritmo permite muestrear la distribución posterior y realizar inferencias sobre los parámetros sin tener que calcular la integral de la evidencia marginal, haciendo viable el análisis bayesiano en escenarios de alta dimensionalidad donde las soluciones cerradas son imposibles.\n\nPosterior beta analítica\nSea una moneda con 10 lanzamientos y 7 caras, para obtener una posterior \\(\\text{Beta}(8,4)\\) a partir de 7 caras (\\(k=7\\)) y 3 cruces (\\(n-k=3\\)), debemos asumir un prior Uniforme o \\(\\text{Beta}(1,1)\\). En la inferencia Bayesiana, demode que si se usa un Prior Beta y un Likelihood Binomial, el Posterior siempre es otra Beta:\n\nLikelihood (Verosimilitud): \\(P(X|\\theta) \\propto \\theta^7 (1-\\theta)^3\\)\nPrior (A priori): \\(P(\\theta) \\sim \\text{Beta}(1,1) \\propto 1\\)\nPosterior (A posteriori): \\(P(\\theta|X) \\propto \\theta^{7+1-1} (1-\\theta)^{3+1-1} = \\theta^7 (1-\\theta)^3 \\rightarrow \\text{Beta}(8,4)\\)\n\nEn términos de Python:\n\n# Funciones del modelo\n\ndef log_prior(theta):\n    \"\"\"\n    Prior Beta(1,1) (Uniforme en [0, 1]).\n    Retorna 0 si está en rango (log(1)), -inf si no.\n    \"\"\"\n    if 0 &lt;= theta &lt;= 1:\n        return 0.0\n    return -np.inf\n\ndef log_likelihood(theta, heads, trials):\n    \"\"\"\n    Log-Likelihood Binomial.\n    \"\"\"\n    if theta &lt; 0 or theta &gt; 1:\n        return -np.inf\n    \n    # Proporcional a theta^k * (1-theta)^(n-k)\n    term_heads = heads * np.log(theta)\n    term_tails = (trials - heads) * np.log(1 - theta)\n    \n    return term_heads + term_tails\n\ndef log_posterior(theta, heads, trials):\n    lp = log_prior(theta)\n    \n    if not np.isfinite(lp):\n        return -np.inf\n        \n    return lp + log_likelihood(theta, heads, trials)\n\nComo el objetivo es obtener exactamente \\(\\text{Beta}(8,4)\\) con 7 caras y 3 cruces, implícitamente se está asumiendo que no se aporta información previa (el 1 inicial de la Beta actúa como un “neutro” o punto de partida cero en términos de influencia).\nConfiguramos el problema en Python:\n\n# CONFIGURACIÓN DEL PROBLEMA\n# Establecer semilla\nnp.random.seed(rnd_seed)\n\n# Datos observados: 10 lanzamientos, 7 caras\nn_trials = 10\nn_heads = 7\n\n# Parámetros del algoritmo MCMC\nn_chains = 4      # Solicitado para diagnóstico R-hat\nn_samples = 5000  # Muestras por cadena\nburnin = 1000     # Periodo de calentamiento\nstep_size = 0.1   # Desviación estándar (Proposal width)\n\n# DEFINICIONES AUXILIARES\ndef log_posterior(theta, heads, trials):\n    \"\"\"\n    Calcula el log-posterior (Likelihood Binomial + Prior Uniforme).\n    Necesario para el algoritmo MH.\n    \"\"\"\n    # Prior Uniforme [0, 1]\n    if theta &lt; 0 or theta &gt; 1:\n        return -np.inf\n    \n    # Log-Likelihood Binomial\n    # P(D|theta) = theta^h * (1-theta)^(n-h)\n    ll = (heads * np.log(theta) + \n          (trials - heads) * np.log(1 - theta))\n    return ll\n\nImplmentamos el algoritmo de Metropolis-Hastings para muestrear la posterior:\n\n# ALGORITMO METROPOLIS-HASTINGS\ndef metropolis_hastings(n_samples, n_chains, heads, \n                        trials, step_size):\n    chains = []\n    print(f\"Iniciando muestreo con {n_chains} cadenas...\")\n\n    for chain_idx in range(n_chains):\n        samples = []\n        \n        # Punto de inicio aleatorio\n        current_theta = np.random.uniform(0.1, 0.9)\n        \n        # Evaluar estado inicial\n        current_log_post = log_posterior(\n            current_theta, heads, trials\n        )\n\n        # Bucle de muestreo (+ burnin)\n        for i in range(n_samples + burnin):\n            # 1. Propuesta: Random Walk\n            # (Normal centrada en theta actual)\n            proposal = np.random.normal(current_theta, step_size)\n\n            # 2. Log-Posterior de la propuesta\n            proposal_log_post = log_posterior(\n                proposal, heads, trials\n            )\n\n            # 3. Ratio de Aceptación (log scale)\n            # log(r) = log(p(new)) - log(p(old))\n            log_ratio = proposal_log_post - current_log_post\n\n            # 4. Decisión de aceptación\n            accept_threshold = np.log(np.random.rand())\n            \n            if accept_threshold &lt; log_ratio:\n                current_theta = proposal\n                current_log_post = proposal_log_post\n\n            # Guardar solo después del burn-in\n            if i &gt;= burnin:\n                samples.append(current_theta)\n\n        chains.append(samples)\n        print(f\"Cadena {chain_idx + 1} completada.\")\n\n    return np.array(chains)\n\nUtilizamos la implementación para obtener muestras de la posterior:\n\n# EJECUCIÓN\nchains = metropolis_hastings(\n    n_samples, n_chains, n_heads, n_trials, step_size\n)\n\n# Convertir a objeto InferenceData de Arviz\nidata = az.from_dict(posterior={\"theta\": chains})\n\nIniciando muestreo con 4 cadenas...\nCadena 1 completada.\nCadena 2 completada.\nCadena 3 completada.\nCadena 4 completada.\n\n\n\n\n\nTabla 4 - Resumen ArviZ: Media Estimada \\(\\theta \\approx 0.6680\\)\n\n\nIntervalo HDI (3% - 97%): [0.420, 0.897]\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\ntheta\n0.668\n0.131\n0.42\n0.897\n0.003\n0.002\n1691.0\n1971.0\n1.0\n\n\n\n\n\n\n\n\nDiagnósticos Obligtatorios\n\nTraceplot\nEl Traceplot permite visualizar la evolución temporal de las cadenas de Markov, facilitando la verificación inmediata de la convergencia asintótica y la calidad del muestreo. A través de este gráfico se evalúa si el algoritmo ha alcanzado la estacionariedad (oscilando alrededor de un valor estable sin tendencias) y si existe una mezcla adecuada del espacio de parámetros (exploración eficiente), lo cual es indispensable para validar que las muestras obtenidas son representativas de la distribución posterior objetivo y descartar problemas como una alta autocorrelación o una configuración errónea del tamaño de paso.\n\n\n\n\n\n\n\n\n\nEl gráfico confirma la convergencia exitosa y robusta del algoritmo Metropolis-Hastings, validando la calidad de la inferencia realizada. En el panel derecho (Traceplot), se observa una oscilación densa y constante alrededor de un eje central sin tendencias visibles —patrón “oruga peluda”—, lo cual demuestra que las cuatro cadenas han alcanzado la estacionariedad y exploran el espacio de parámetros con una mezcla eficiente y homogénea. Corroborando esto, el panel izquierdo (Densidad Posterior) exhibe una superposición casi perfecta de las distribuciones estimadas por cada cadena, indicando consistencia interna (bajo R-hat) y revelando una moda centrada aproximadamente en 0.7, valor que coincide con el máximo teórico esperado para la distribución Beta(8,4).\n\n\nAutocorrelación\nEl análisis de la autocorrelación permite cuantificar la dependencia serial inherente entre las muestras generadas por la cadena de Markov, dado que los algoritmos MCMC producen, por definición, observaciones correlacionadas en lugar de independientes. Este diagnóstico es crítico para evaluar la eficiencia del muestreo (mixing), ya que una persistencia alta de la correlación a través de múltiples retardos (lags) indica una exploración lenta del espacio de parámetros y reduce el tamaño de muestra efectivo (ESS), lo que alertaría sobre la necesidad de incrementar el número de iteraciones o ajustar el tamaño de paso para garantizar que la inferencia estadística sobre la posterior sea fiable y precisa.\n\n\n\n\n\n\n\n\n\nLos gráficos de autocorrelación revelan una persistente dependencia serial entre las muestras sucesivas, evidenciada por un decaimiento lento de las barras que mantienen valores significativos incluso después de 15 a 20 retardos (lags). Este comportamiento, consistente en las cuatro cadenas analizadas, indica una eficiencia de mezcla (mixing) moderada y una exploración del espacio de parámetros con cierta “viscosidad”, lo cual reduce el Tamaño de Muestra Efectivo (ESS) e implica que, para fines de inferencia estadística robusta, la cantidad de información independiente real es considerablemente menor al número total de iteraciones computadas.\n\n\nHistograma vs Analítica Beta(8,4)\nLa superposición del histograma de frecuencias empíricas frente a la curva analítica de la distribución Beta es una prueba definitiva de validación, aprovechando que el modelo Beta-Binomial posee una solución cerrada conocida que sirve como “verdad fundamental” (ground truth). Este diagnóstico visual permite confirmar rigurosamente que el algoritmo Metropolis-Hastings está muestreando fielmente de la distribución objetivo y no de una aproximación errónea, verificando así que la lógica computacional, la función de verosimilitud y el mecanismo de aceptación/rechazo han sido implementados con exactitud matemática y que el muestreador recupera la geometría correcta de la posterior sin sesgos.\n\n\n\n\n\n\n\n\n\nEl gráfico evidencia una correspondencia altamente satisfactoria entre la distribución empírica generada por el algoritmo (histograma azul) y la solución analítica teórica (curva roja), demostrando la exactitud de la implementación. La alineación precisa de las frecuencias de las barras con el perfil de la función de densidad Beta(8,4) confirma que el muestreador ha logrado capturar fielmente la estructura probabilística de la posterior, reproduciendo correctamente tanto la ubicación de la moda en torno a 0.7 como la dispersión asociada, lo cual valida que las muestras obtenidas son estadísticamente representativas de la distribución objetivo y legitima el uso de la simulación para la inferencia paramétrica.\n\n\nR-hat con 4 cadenas y ESS (Effective Sample Size)\nLas métricas R-hat y ESS (Tamaño de Muestra Efectivo) proporcionan una validación cuantitativa y objetiva de la fiabilidad y precisión de la simulación, superando las limitaciones de la inspección visual. El estadístico R-hat resulta indispensable al utilizar múltiples cadenas para verificar matemáticamente la convergencia global, asegurando a través de la comparación de varianzas que el algoritmo no ha quedado atrapado en óptimos locales; simultáneamente, el ESS es crítico para cuantificar el volumen real de información independiente generada descontando la autocorrelación, garantizando así que la estimación de la posterior posea un error estándar de Monte Carlo lo suficientemente reducido para ser estadísticamente válida.\n\n\n\n\n\nTabla 5 - Diagnóstico Numérico (MCMC)\nParámetro analizado: theta\n\n\n\nMétrica\nValor\nCriterio\nEstado\n\n\n\n\nR-hat (Gelman-Rubin)\n1.0000\n&lt; 1.01\nESTABLE\n\n\nESS (Bulk)\n1691\n&gt; 400\nSUFICIENTE\n\n\n\n\n\nLos diagnósticos numéricos validan la robustez técnica de la simulación, destacando un estadístico R-hat de 1.0 que confirma la convergencia perfecta de las cadenas y su indistinguibilidad estadística. Por su parte, el Tamaño de Muestra Efectivo (ESS) de 1691.0, aunque inferior al total de iteraciones debido a la correlación serial, resulta suficiente para garantizar estimaciones estables de la tendencia central, lo cual, sumado a un error estándar de Monte Carlo (mcse) marginal de 0.003, asegura que la precisión de la inferencia sobre la posterior es elevada y que el error introducido por el método de muestreo es despreciable.\n\nParte 3 - Simulación de Eventos Discretos (M/M/1 o M/M/c)\nPara implementar esta simulación, el motor central debe basarse en una Lista de Eventos Futuros (LEF) ordenada cronológicamente y una variable de reloj (\\(T_{now}\\)). El tiempo no avanza de forma continua, sino que “salta” discretamente al instante del evento más próximo en la lista. Inicialmente, se genera la primera llegada aleatoria (usando la tasa \\(\\lambda=10\\)) y se inserta en la LEF; el ciclo de simulación consiste en extraer repetidamente el evento de menor tiempo de la lista, actualizar el reloj a ese instante y ejecutar la lógica de cambio de estado correspondiente hasta que \\(T_{now}\\) supere las 8 horas.\nLa lógica de estado maneja dos eventos principales: Llegada y Salida. Al procesar una Llegada, se programa inmediatamente la siguiente llegada futura y se evalúan los servidores: si hay alguno libre (de los \\(c\\) disponibles), se ocupa y se calcula una duración de servicio (con tasa \\(\\mu=4\\)) para insertar un evento de Salida en la LEF; si todos están ocupados, se incrementa el contador de la cola. Por otro lado, al procesar una Salida, se libera el servidor, pero si la cola no está vacía, se decrementa inmediatamente para ingresar al siguiente cliente al servicio, generando su respectivo evento de finalización futuro.\n\nSimulación 8 horas de un sistemas de colas\nEsta es una implementaciónen Python de una simulación de eventos discretos (DES) utilizando una Lista de Eventos Futuros (LEF) implementada con una cola de prioridad (heapq). El código está diseñado para soportar tanto M/M/1 (1 servidor) como M/M/c (múltiples servidores), pero además tenemos estos parámetros:\n\n\\(\\lambda\\) (Tasa de llegadas): 10 clientes/hora.\n\\(\\mu\\) (Tasa de servicio): 4 clientes/hora.\n\\(c\\) (Servidores): Variable.\n\nEs importante resaltar que si usamos \\(c=1\\) (M/M/1), el sistema será inestable porque la tasa de llegada (10) es mayor que la capacidad de servicio (4). La cola crecerá infinitamente. Para un sistema estable, necesitamos \\(c \\ge 3\\) (capacidad 12 &gt; 10). Por esto se ha configurado el código por defecto con NUM_SERVIDORES = 3, pero puede cambiarse a 1 para observar cómo se satura.\nEn pimera instancia, definimos las clases y funciones necesarias para la simulación:\n\nclass SimulacionMMC_Core:\n    def __init__(self, tiempo_max, tasa_llegada, \n                 tasa_servicio, n_servidores):\n        self.tiempo_max = tiempo_max\n        self.tasa_llegada = tasa_llegada\n        self.tasa_servicio = tasa_servicio\n        self.n_servidores = n_servidores\n\n        # Estado del sistema\n        self.reloj = 0.0\n        self.num_en_cola = 0\n        self.servidores_ocupados = 0\n        self.lef = []  # Lista de Eventos Futuros\n\n        # Estadísticas\n        self.total_llegadas = 0\n        self.total_atendidos = 0\n        self.area_cola = 0.0\n        self.area_ocupados = 0.0\n        self.tiempo_ultimo_evento = 0.0\n\n        # Historial\n        self.historia = [(0.0, 0, 0)]\n\n    def actualizar_estadisticas(self, tiempo_actual):\n        delta_t = tiempo_actual - self.tiempo_ultimo_evento\n        self.area_cola += self.num_en_cola * delta_t\n        self.area_ocupados += self.servidores_ocupados * delta_t\n        self.tiempo_ultimo_evento = tiempo_actual\n\n    def procesar_llegada(self):\n        self.total_llegadas += 1\n        prox = self.reloj + random.expovariate(self.tasa_llegada)\n        heapq.heappush(self.lef, (prox, 0))\n\n        if self.servidores_ocupados &lt; self.n_servidores:\n            self.servidores_ocupados += 1\n            duracion = random.expovariate(self.tasa_servicio)\n            t_salida = self.reloj + duracion\n            heapq.heappush(self.lef, (t_salida, 1))\n        else:\n            self.num_en_cola += 1\n\n    def procesar_salida(self):\n        self.total_atendidos += 1\n        if self.num_en_cola &gt; 0:\n            self.num_en_cola -= 1\n            duracion = random.expovariate(self.tasa_servicio)\n            t_salida = self.reloj + duracion\n            heapq.heappush(self.lef, (t_salida, 1))\n        else:\n            self.servidores_ocupados -= 1\n\n    def correr(self):\n        # Programar primera llegada\n        t_primera = random.expovariate(self.tasa_llegada)\n        heapq.heappush(self.lef, (t_primera, 0))\n\n        while self.reloj &lt; self.tiempo_max and self.lef:\n            tiempo_evento, tipo_evento = heapq.heappop(self.lef)\n\n            if tiempo_evento &gt; self.tiempo_max:\n                self.actualizar_estadisticas(self.tiempo_max)\n                self.reloj = self.tiempo_max\n                break\n\n            self.actualizar_estadisticas(tiempo_evento)\n            self.reloj = tiempo_evento\n\n            if tipo_evento == 0:\n                self.procesar_llegada()\n            else:\n                self.procesar_salida()\n\n            self.historia.append((\n                self.reloj,\n                self.num_en_cola,\n                self.servidores_ocupados\n            ))\n\n        # Cálculos finales\n        lq = self.area_cola / self.reloj\n        prom_ocupados = self.area_ocupados / self.reloj\n        utilizacion = prom_ocupados / self.n_servidores\n        lambda_real = self.total_llegadas / self.reloj\n        wq = lq / lambda_real if lambda_real &gt; 0 else 0\n\n        # Retornar diccionario de resultados\n        return {\n            \"tiempo_simulado\": self.reloj,\n            \"total_llegadas\": self.total_llegadas,\n            \"total_atendidos\": self.total_atendidos,\n            \"lq\": lq,\n            \"wq\": wq,\n            \"utilizacion\": utilizacion,\n            \"prom_ocupados\": prom_ocupados,\n            \"historia\": self.historia,\n            \"capacidad\": self.n_servidores\n        }\n\nA continuación se utiliza la clase de Simulación M/M/c implementada. Si n_servidores=1 la cola crece infinitamente, esto ese produce un cuello de botella\n\n# Configurar la simulación (8 horas, lambda=10, mu=4, c=3)\nrandom.seed(rnd_seed)\n\nsim = SimulacionMMC_Core(\n    tiempo_max=8, \n    tasa_llegada=10,  # Lambda\n    tasa_servicio=4,  # Mu\n    n_servidores=3    # c\n)\n\ndatos = sim.correr()\n\n\n\n\n\nTabla 6 - Resultados de la Simulación M/M/3\n\n\n\nMétrica\nValor\nDescripción\n\n\n\n\nTiempo Simulado\n8.00 h\nDuración total\n\n\nLlegadas Totales\n84\nClientes que entraron\n\n\nAtendidos\n66\nClientes completados\n\n\nLq (Cola prom)\n2.7528\nLongitud media de cola\n\n\nWq (Tiempo cola)\n15.73 min\nTiempo medio espera\n\n\nUtilización (\\(\\rho\\))\n85.85%\nSATURADO\n\n\n\n\n\nPara visualizar los resultados de la simulación de eventos discretos se usan gráficos de paso (step plots) dad que el estado del sistema (número en cola en espera o servidores ocupados) permanece constante entre eventos y va cambiando con cada arribo.\n\n\n\n\n\n\n\n\n\n\nParte 4 - Modelos Continuos\nPara implementar un modelo de simulación de eventos basado en un Proceso de Poisson No Homogéneo (NHPP) mediante el método de Thinning (o adelgazamiento de Lewis-Shedler), el primer paso consiste en definir una tasa mayorante constante \\(\\lambda^*\\), la cual debe ser igual o superior al valor máximo que alcanza la función de intensidad variable \\(\\lambda(t)\\) durante todo el periodo de simulación. El motor de simulación avanza generando una secuencia de tiempos de arribo “candidatos” utilizando esta tasa máxima constante, creando efectivamente un proceso de Poisson homogéneo que “sobremuestrea” la línea de tiempo con más eventos de los necesarios.\nEl segundo paso es el proceso de filtrado estocástico que da nombre al método. Para cada evento candidato generado en el instante \\(t\\), se evalúa si se conserva o se descarta mediante una prueba de aceptación-rechazo: se genera un número aleatorio uniforme \\(u \\in [0, 1]\\) y se compara con el ratio \\(\\lambda(t) / \\lambda^*\\). Si \\(u\\) es menor o igual a esta proporción, el evento se acepta y se procesa; de lo contrario, se ignora (se “adelgaza” la secuencia). De esta forma, la probabilidad de aceptar un evento es proporcional a la intensidad real en ese momento, resultando en una distribución de eventos que se ajusta fielmente a la curva de la tasa variable \\(\\lambda(t)\\).\n\nImplementación de NHPPP por Thinning\nAsí para esta simulación se diseña una función de intensidad \\(\\lambda(t)\\) que es realista para una simulación de 7 días: simula ciclos diarios (como el tráfico de una web o clientes en una tienda) con picos durante el día y valles durante la noche. El algoritmo de Thinning (o Aceptación-Rechazo) funciona en tres pasos:\n\nDominancia: Se define una tasa constante \\(\\lambda_{max}\\) que sea mayor o igual a la tasa real \\(\\lambda(t)\\) en todo momento.\nGeneración: Se generan “candidatos” a eventos usando un Proceso de Poisson Homogéneo con la tasa máxima \\(\\lambda_{max}\\).\nFiltrado (Thinning): Se acepta cada candidato como un evento real con probabilidad \\(P = \\frac{\\lambda(t)}{\\lambda_{max}}\\).\n\n\n# Función de Intensidad\ndef intensity_function(t):\n    \"\"\"\n    Define la tasa de llegada lambda(t).\n    Ciclo diario (24h) con picos al mediodía.\n    \"\"\"\n    cycle = 15 * np.sin(2 * np.pi * (t - 9) / 24)\n    rate = 20 + cycle\n    return max(0, rate)\n\n# Función de Filtro\ndef simulate_nhpp_thinning(t_max, lambda_upper_bound):\n    \"\"\"\n    Simulación NHPP mediante Thinning.\n    Retorna: (lista de eventos, número total de intentos)\n    \"\"\"\n    t = 0\n    events = []\n    candidates_count = 0\n    \n    while t &lt; t_max:\n        # 1. Generar candidato (Poisson Homogéneo)\n        u1 = np.random.uniform(0, 1)\n        w = -np.log(u1) / lambda_upper_bound\n        t = t + w\n        \n        if t &gt;= t_max:\n            break\n            \n        candidates_count += 1\n            \n        # 2. Probabilidad de aceptación\n        prob_acceptance = intensity_function(t) / lambda_upper_bound\n        \n        # 3. Test de aceptación (Thinning)\n        u2 = np.random.uniform(0, 1)\n        if u2 &lt;= prob_acceptance:\n            events.append(t)\n            \n    return np.array(events), candidates_count\n\nA continuación estas funciones se usan en una simulación de 7 días o 168 horas:\n\n# Parámetros \nDIAS = 7\nHORAS_TOTALES = DIAS * 24\nLAMBDA_MAX = 35 \n\n# Ejecución\nevents, candidates = simulate_nhpp_thinning(HORAS_TOTALES, LAMBDA_MAX)\n\n\n\n\n\nTabla 7 - Reporte de Simulación (NHPP - 7 Días)\n\n1. Métricas Generales\n\n\n\nMétrica\nValor\n\n\n\n\nTiempo Total\n168 horas\n\n\nEventos Generados\n3294\n\n\nCandidatos\n5776\n\n\nEficiencia Thinning\n57.03%\n\n\nPromedio Global\n19.61 ev/h\n\n\n\n\n\n2. Tiempos entre llegadas (IAT)\n\n\n\nEstadístico\nValor\n\n\n\n\nPromedio\n0.0510 h (3.1 min)\n\n\nMínimo\n0.000020 h\n\n\nMáximo\n1.1459 h\n\n\nDesv. Std\n0.0724\n\n\n\n\n\n3. Desglose Diario\n\n\n\nDía\nRango\nEventos\nPromedio (ev/h)\n\n\n\n\n1\n000 - 024 h\n487\n20.29\n\n\n2\n024 - 048 h\n454\n18.92\n\n\n3\n048 - 072 h\n442\n18.42\n\n\n4\n072 - 096 h\n479\n19.96\n\n\n5\n096 - 120 h\n472\n19.67\n\n\n6\n120 - 144 h\n469\n19.54\n\n\n7\n144 - 168 h\n491\n20.46\n\n\n\n\n\n\nSe visualizan los resultados de la simulación:\n\n\n\n\n\n\n\n\n\nLa visulización ilustra la evolución temporal de un Proceso de Poisson No Homogéneo (NHPP) durante un periodo de 168 horas, equivalente a siete días completos. El componente principal es la curva azul oscuro que representa la función de intensidad \\(\\lambda(t)\\), la cual exhibe un comportamiento perfectamente cíclico y sinusoidal, oscilando entre una tasa base cercana a 5 y un pico máximo de 35 eventos por hora. Sobre esta curva se proyecta una línea roja discontinua que marca la cota superior \\(\\lambda_{max}\\) (establecida en 35), la cual actúa como el techo de referencia necesario para la generación de eventos candidatos dentro del algoritmo de Thinning.\nEn la franja inferior, las líneas verticales verdes denotan los instantes exactos de los eventos finalmente aceptados por el modelo. Se observa una correlación directa entre la densidad de estas líneas y la magnitud de la función de intensidad: la concentración de eventos se vuelve densa y compacta coincidiendo con los picos de la onda sinusoidal, mientras que se dispersa notablemente durante los valles. Esta distribución visual confirma la eficacia del método de aceptación-rechazo, demostrando que la frecuencia de ocurrencia de los eventos se modula dinámicamente en función de la tasa variable \\(\\lambda(t)\\) en cada instante del tiempo.\n\nParte 5 - Gillespie SSA o Next Reaction Method\nPara implementar el algoritmo de Gillespie (SSA) en este sistema, primero se deben calcular en cada iteración las propensiones (\\(a_v\\)) que determinan la probabilidad instantánea de cada canal de reacción. Para la síntesis (orden cero), la propensión es constante, \\(a_1 = k_1 = 10\\); para la degradación (primer orden), la propensión depende del estado actual de la población, \\(a_2 = k_2 \\times [\\text{mRNA}]\\). Se calcula la propensión total \\(a_0 = a_1 + a_2\\) y se genera el tiempo hasta el próximo evento (\\(\\tau\\)) muestreando una distribución exponencial con media \\(1/a_0\\) (usualmente \\(-\\ln(u_1)/a_0\\)), lo que define cuánto tiempo transcurre en el sistema antes de que la configuración molecular cambie.\nUna vez determinado el “cuándo”, se decide el “qué” seleccionando una de las dos reacciones con probabilidad proporcional a su magnitud relativa (\\(P_{síntesis} = a_1/a_0\\) y \\(P_{degradación} = a_2/a_0\\)). Dependiendo de la reacción elegida, se actualiza el contador de moléculas de mRNA incrementándolo o decrementándolo en una unidad, se avanza el tiempo de simulación (\\(t \\leftarrow t + \\tau\\)) y, crucialmente, se recalculan las propensiones para el siguiente paso, dado que \\(a_2\\) cambiará cada vez que varíe la cantidad de mRNA, capturando así las fluctuaciones estocásticas intrínsecas del sistema.\n\nAlgoritmo de Gillespie (Método Directo)\nEsta es una implementación del Algoritmo de Gillespie (Método Directo) en Python para el sistema de nacimiento y muerte (producción y degradación de ARNm) planteado:\n\\[\\to \\text{mRNA} (k_{1}=10)\\] \\[\\text{mRNA}\\to (k_{2}=1)\\]\nBasado en esta descripción tenemos dos reacciones: 1. Producción (Transcripción): \\(\\emptyset \\xrightarrow{k_1} \\text{mRNA}\\). Tasa constante: \\(k_1 = 10\\). Esta reacción incrementa el conteo de ARNm en 1. 3. Degradación: \\(\\text{mRNA} \\xrightarrow{k_2} \\emptyset\\). Tasa dependiente de la cantidad actual: \\(k_2 = 1\\). Esta reacción disminuye el conteo de ARNm en 1.\nCabe destacar que el estado estacionario promedio esperado es \\(k_1 / k_2 = 10\\) moléculas. El estado estacionario promedio de \\(10\\) moléculas se fundamenta en el principio de equilibrio dinámico, el cual se alcanza cuando la velocidad de entrada (producción) iguala a la velocidad de salida (degradación) del sistema. Dado que la producción ocurre a una tasa constante \\(k_1\\) y la degradación es proporcional a la cantidad de moléculas presentes (\\(k_2 \\cdot \\text{mRNA}\\)), el balance se logra matemáticamente cuando \\(k_1 = k_2 \\cdot \\text{mRNA}\\); al despejar la concentración de equilibrio, se obtiene el cociente \\(k_1 / k_2\\), lo que resulta en un valor promedio de \\(10\\) moléculas para los parámetros dados.\n\n# Funcion de Simulacion Gillespie SSA\ndef gillespie_ssa(k1, k2, t_max):\n    # Inicialización\n    t = 0.0\n    # Condición inicial (puedes cambiarla)\n    mRNA = 0\n    \n    # Listas para guardar el historial (para graficar)\n    time_points = [t]\n    mRNA_counts = [mRNA]\n    \n    while t &lt; t_max:\n        # 1. Calcular las propensiones (propensities)\n        # a1: Probabilidad de producción (constante)\n        a1 = k1\n        # a2: Prob. degradación (proporcional a moléculas)\n        a2 = k2 * mRNA\n        \n        a_sum = a1 + a2\n        \n        # Si a_sum es 0, el sistema para (sin reacciones)\n        if a_sum == 0:\n            break\n            \n        # 2. Determinar tiempo hasta próxima reacción (tau)\n        # Se extrae de una distribución exponencial\n        r1 = np.random.rand()\n        tau = (1.0 / a_sum) * np.log(1.0 / r1)\n        \n        # 3. Determinar qué reacción ocurre\n        r2 = np.random.rand()\n        \n        if r2 &lt; (a1 / a_sum):\n            # Ocurre Reacción 1: Producción\n            mRNA += 1\n        else:\n            # Ocurre Reacción 2: Degradación\n            mRNA -= 1\n            \n        # 4. Actualizar tiempo y guardar estado\n        t += tau\n        time_points.append(t)\n        mRNA_counts.append(mRNA)\n        \n    return time_points, mRNA_counts\n\nA continuación se usa la simulación para el sistema planteado:\n\n# Parámetros\n# Dado que se simulan ARNm, los procesos de transcripción y \n# degradación suelen medirse en minutos o horas.\nk1 = 10.0\nk2 = 1.0\nt_max = 1000.0 \n\n# Simular\ntiempos, conteos = gillespie_ssa(k1, k2, t_max)\n\n\n\n\n\nTabla 8 - Resultados Simulación Estocástica (Gillespie SSA)\n\n1. Parámetros del Sistema\n\n\n\nParámetro\nValor\n\n\n\n\nProducción (\\(k_1\\))\n10.0\n\n\nDegradación (\\(k_2\\))\n1.0\n\n\nTiempo Total\n1000.00 u.t.\n\n\nEventos Totales\n20185\n\n\n\n\n\n2. Análisis de Trayectoria (Estadística)\n\n\n\nMétrica\nValor Observado\nRango\n\n\n\n\nMedia (\\(\\mu\\))\n10.7573\n-\n\n\nDesv. Std (\\(\\sigma\\))\n3.3082\n-\n\n\nVarianza (\\(\\sigma^2\\))\n10.9444\n-\n\n\nFluctuación\n-\n[0 - 24]\n\n\n\n\n\n3. Validación Teórica (vs Poisson)\n\n\n\nConcepto\nEsperado\nObservado\nError\n\n\n\n\nEstado Estacionario\n10.0000\n10.7573\nNOT OK 7.57%\n\n\nVarianza\n10.0000\n10.9444\n-\n\n\n\n\n\n\nSe visualizan los resultados de la simulación:\n\n\n\n\n\n\n\n\n\nLa visualización ilustra la naturaleza estocástica del sistema de producción y degradación de ARNm, donde la trayectoria azul exhibe fluctuaciones continuas y aleatorias inherentes al ruido intrínseco del proceso biológico. Se observa que, tras superar la condición inicial nula, el número de moléculas oscila dinámicamente alrededor del valor de equilibrio teórico señalado por la línea discontinua roja (10 moléculas), demostrando que, si bien el promedio temporal del sistema converge al modelo determinista, el estado instantáneo varía constantemente dentro de un rango de dispersión característico (aproximadamente entre 0 y 23 moléculas).\n\nParte Integradora - Simulación de Tráfico Aéreo\nSe propone un modelo de simulación híbrida de tráfico aéreo donde el flujo de entrada de aeronaves se gestiona mediante un Proceso de Poisson No Homogéneo (NHPP) utilizando el método de Thinning. Este componente permite modelar fielmente las curvas de demanda operativa del aeropuerto, generando llegadas estocásticas que respetan los picos horarios (horas punta) y los valles nocturnos. Estas entidades (aviones) ingresan a un sistema de colas modelado bajo la lógica M/M/1, donde la pista de aterrizaje actúa como el servidor único con tiempos de servicio exponenciales, gestionando tanto la cola de espera en el aire (holding pattern) como en tierra (taxiway).\nDe forma paralela y asíncrona, se ejecuta un motor basado en el Algoritmo de Gillespie (SSA) para simular la dinámica meteorológica. En este contexto, los estados del clima (ej. “Despejado”, “Viento Cruzado”, “Tormenta Eléctrica”) se tratan como “especies químicas” discretas que transicionan estocásticamente en función de tasas de cambio históricas (propensidades). El algoritmo de Gillespie determina los intervalos exactos de tiempo en los que el sistema permanece en cada estado climático, generando una línea de tiempo de condiciones ambientales independiente del tráfico aéreo pero estadísticamente exacta.\nLa integración de ambos modelos se realiza mediante una modulación dinámica de la tasa de servicio (\\(\\mu\\)). El estado vigente dictado por el módulo Gillespie actúa como una variable de control sobre el modelo M/M/1: cuando Gillespie transiciona a un estado de “Tormenta”, la tasa de servicio de la pista se reduce drásticamente o se anula (\\(\\mu \\to 0\\)), provocando que las operaciones de aterrizaje y despegue se aborten. Esto fuerza al sistema de colas a entrar en un régimen de saturación o desvío de entidades, permitiendo analizar no solo la congestión por volumen de tráfico, sino la resiliencia del aeropuerto ante ventanas de inoperatividad estocástica y su capacidad de recuperación (recovery rate) una vez restablecidas las condiciones favorables.\nEl pipeline de ejecución se orquesta mediante un reloj maestro que sincroniza dos generadores de eventos concurrentes: un módulo de tráfico que inyecta entidades (aeronaves) en la línea de tiempo usando NHPP por Thinning para replicar la demanda horaria, y un motor Gillespie independiente que actualiza asincrónicamente el estado global del clima (variables de entorno). Dentro del bucle principal de procesamiento (LEF), la lógica del servidor M/M/1 consulta en tiempo real el estado vigente dictado por Gillespie antes de intentar atender a una aeronave; si el motor climático indica un evento adverso (ej. “Tormenta”), se activa una interrupción que anula o penaliza drásticamente la tasa de servicio (\\(\\mu\\)), forzando la acumulación de entidades en la cola hasta que una nueva transición estocástica del clima restablezca la operatividad, permitiendo así medir el impacto sistémico de la interrupción.\nEn primer lugar configuramos los parámetros globales de la simulación:\n\n# CONFIGURACIÓN Y PARÁMETROS\nSIM_DURATION = 24.0  # Horas\nSEED = rnd_seed\n\n# --- Parámetros de Tráfico (NHPP) ---\n# Tasa máxima de llegadas (aviones/hora) para Thinning\nLAMBDA_MAX = 20.0\n# Tasa de servicio de la pista (aviones/hora) en buen clima\nMU_OPERATIVO = 25.0\n\n# --- Parámetros Climáticos (Gillespie) ---\n# Estado 0: Despejado, Estado 1: Tormenta\n# Frecuencia de aparición de tormentas\nTASA_DESPEJADO_A_TORMENTA = 0.15\n# Velocidad de recuperación (duración tormenta)\nTASA_TORMENTA_A_DESPEJADO = 0.8\n\n# --- Tipos de Eventos ---\nEVT_ARRIVAL = 0\nEVT_DEPARTURE = 1\nEVT_WEATHER = 2\n\n# --- Estados del Clima ---\nCLIMA_CLEAR = 0\nCLIMA_STORM = 1\n\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\nA continuación se implementa en un clase la simulación híbrida con los dos motores concurrentes (NHPP + Gillespie SSA), estructurando la lógica de eventos y la interacción entre ambos modelos:\n\nclass HybridAirportSim:\n    def __init__(self, t_max):\n        self.t_max = t_max\n        self.clock = 0.0\n\n        # Estado del Sistema\n        self.queue_count = 0        # En cola (aire + tierra)\n        self.server_busy = False    # Estado de la pista\n        self.weather_state = CLIMA_CLEAR\n\n        # Gestión de Eventos\n        self.events = []  # Heapq\n        self.current_departure_token = None\n\n        # Estadísticas\n        self.stats = {\n            'arrivals': 0, 'completed': 0,\n            'aborted_ops': 0, 'weather_changes': 0\n        }\n\n        # Historiales\n        self.history_occupancy = [(0.0, 0)]\n        self.history_weather = [(0.0, CLIMA_CLEAR)]\n        self.history_aborts = [] \n\n    def schedule_event(self, time, event_type, token=None):\n        if time &lt;= self.t_max:\n            heapq.heappush(self.events, (time, event_type, token))\n\n    # --- MÓDULO NHPP (Tráfico) ---\n    def get_arrival_rate(self, t):\n        # Base de 5 + oscilación\n        cycle = np.sin((t - 6) * np.pi / 12) ** 2\n        return 5 + 15 * cycle\n\n    def schedule_next_arrival_nhpp(self):\n        t_curr = self.clock\n        while True:\n            u1 = random.random()\n            dt = -np.log(u1) / LAMBDA_MAX\n            t_curr += dt\n\n            if t_curr &gt; self.t_max: return\n\n            # Thinning (Aceptación/Rechazo)\n            lambda_t = self.get_arrival_rate(t_curr)\n            if random.random() &lt;= (lambda_t / LAMBDA_MAX):\n                self.schedule_event(t_curr, EVT_ARRIVAL)\n                break\n\n    # --- MÓDULO CLIMÁTICO (Gillespie) ---\n    def schedule_next_weather_change(self):\n        if self.weather_state == CLIMA_CLEAR:\n            a0 = TASA_DESPEJADO_A_TORMENTA\n        else:\n            a0 = TASA_TORMENTA_A_DESPEJADO\n\n        if a0 &gt; 0:\n            r = random.random()\n            tau = (1.0 / a0) * np.log(1.0 / r)\n            self.schedule_event(self.clock + tau, EVT_WEATHER)\n\n    # --- CONTROL DE EVENTOS ---\n    def handle_arrival(self):\n        self.stats['arrivals'] += 1\n        self.schedule_next_arrival_nhpp()\n\n        cond_free = not self.server_busy\n        cond_weather = (self.weather_state == CLIMA_CLEAR)\n\n        if cond_free and cond_weather:\n            self.server_busy = True\n            self.schedule_departure()\n        else:\n            self.queue_count += 1\n\n    def schedule_departure(self):\n        s_time = random.expovariate(MU_OPERATIVO)\n        token = random.randint(0, 1000000000)\n        self.current_departure_token = token\n        self.schedule_event(self.clock + s_time, EVT_DEPARTURE, token)\n\n    def handle_departure(self, token):\n        if token != self.current_departure_token: return\n        \n        self.stats['completed'] += 1\n        self.server_busy = False\n        self.current_departure_token = None\n\n        if self.queue_count &gt; 0 and self.weather_state == CLIMA_CLEAR:\n            self.queue_count -= 1\n            self.server_busy = True\n            self.schedule_departure()\n\n    def handle_weather_change(self):\n        self.stats['weather_changes'] += 1\n        self.weather_state = 1 - self.weather_state\n\n        if self.weather_state == CLIMA_STORM:\n            if self.server_busy:\n                # Abortar operación\n                self.stats['aborted_ops'] += 1\n                self.server_busy = False\n                self.queue_count += 1\n                self.current_departure_token = None\n                self.history_aborts.append((self.clock, self.queue_count + 1))\n        \n        elif self.weather_state == CLIMA_CLEAR:\n            if self.queue_count &gt; 0 and not self.server_busy:\n                self.queue_count -= 1\n                self.server_busy = True\n                self.schedule_departure()\n        \n        self.schedule_next_weather_change()\n\n    def run(self):\n        self.schedule_next_arrival_nhpp()\n        self.schedule_next_weather_change()\n\n        while self.events:\n            time, type, token = heapq.heappop(self.events)\n            self.clock = time\n\n            # Registro histórico\n            busy_int = 1 if self.server_busy else 0\n            self.history_occupancy.append((self.clock, self.queue_count + busy_int))\n            self.history_weather.append((self.clock, self.weather_state))\n\n            if type == EVT_ARRIVAL: self.handle_arrival()\n            elif type == EVT_DEPARTURE: self.handle_departure(token)\n            elif type == EVT_WEATHER: self.handle_weather_change()\n\n        # Generar curva de referencia para el gráfico\n        t_ref = np.linspace(0, self.t_max, 200)\n        l_ref = [self.get_arrival_rate(t) for t in t_ref]\n\n        return {\n            \"t_max\": self.t_max,\n            \"stats\": self.stats,\n            \"hist_occupancy\": self.history_occupancy,\n            \"hist_weather\": self.history_weather,\n            \"hist_aborts\": self.history_aborts,\n            \"ref_curve_t\": t_ref,\n            \"ref_curve_y\": l_ref\n        }\n\nY ejecutamos la simulación para un periodo de 24 horas de trafico aéreo:\n\n# EJECUCIÓN DE SIMULACIÓN COMPLETA\nsimulacion = HybridAirportSim(t_max=SIM_DURATION)\nresultados = simulacion.run()\n\n\n\n\nTabla 9 - Reporte de Simulación Híbrida\n\n\n\nMétrica Operativa\nValor\n\n\n\n\nDuración\n24.0 h\n\n\nLlegadas Totales\n304\n\n\nAterrizajes Exitosos\n303\n\n\nOperaciones Abortadas\n1\n\n\n\n\nMeteorología\n\n\n\nMétrica Climática\nValor\n\n\n\n\nCambios de Clima\n6\n\n\nTiempo en Tormenta\n2.84 h (11.8%)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis de Resultados de la Simulación Híbrida y Discusión\nLa interpretación de los datos obtenidos a partir de la simulación híbrida, que integra un proceso de Poisson no homogéneo (NHPP) para la demanda y un motor estocástico de Gillespie para las condiciones meteorológicas, revela dinámicas operativas críticas en la gestión del tráfico aéreo.\n\n\n1. Caracterización de Eventos Meteorológicos Estocásticos\nEl algoritmo de Gillespie generó tres eventos discretos de interrupción meteorológica (indicados como franjas grises verticales en el gráfico generado por la simulación) a lo largo del período simulado de 24 horas, lo que subraya la naturaleza aleatoria del modelo climático implementado:\n\nEvento 1 (03:30 - 04:30 h): Una interrupción de corta duración durante un período de baja demanda operacional.\nEvento 2 (07:00 - 08:00 h): Una interrupción estratégica inmediatamente anterior al primer pico matutino de tráfico aéreo.\nEvento 3 (18:00 - 19:30 h): El evento de mayor duración y criticidad del ciclo diario.\n\n\n\n2. Interacción Crítica entre Demanda y Capacidad: El Escenario de “Tormenta Perfecta”\nEl análisis del gráfico generado identifica una interacción sinérgica adversa alrededor de las 19:00 h, que resultó en una saturación momentánea del sistema:\n\nCoincidencia Temporal: El tercer evento meteorológico coincidió exactamente con el pico máximo de la tasa de llegada (\\(\\lambda\\)) definida por el perfil NHPP (línea amarilla punteada).\nSaturación del Sistema: La conjunción de una capacidad de servicio anulada (\\(\\mu\\) = 0) debido a las condiciones climáticas y una tasa de llegada máxima provocó un incremento vertical y abrupto en la ocupación del sistema (línea azul). El sistema alcanzó su máximo histórico diario, registrando un total de 12 aeronaves en cola.\nVulnerabilidad Operacional: Este resultado demuestra una vulnerabilidad inherente del aeropuerto: la capacidad ociosa durante períodos normales no mitiga el riesgo de congestión exponencial instantánea cuando una interrupción de servicio ocurre durante un pico de demanda.\n\n\n\n3. Dinámica de Operaciones Abortadas (Go-Around)\nSe identificó un caso específico de operación abortada, señalado con una “X” roja al inicio del primer evento de tormenta (aprox. 03:30 h). - Mecanismo de Interrupción: En el instante en que el motor de Gillespie cambió el estado del sistema a “Tormenta”, una aeronave se encontraba en fase de aterrizaje (servidor ocupado). El protocolo del sistema forzó un go-around, cancelando el servicio y reintroduciendo la aeronave en la cola de espera, lo cual se corrobora visualmente por el incremento escalonado inmediato en la ocupación (línea azul).\n\n\n4. Resiliencia y Tasas de Recuperación del Sistema\nLas pendientes descendentes de la línea de ocupación inmediatamente después de la finalización de los eventos climáticos (específicamente tras las 19:30 h) proporcionan métricas clave sobre la resiliencia del sistema: - Rápida Disminución de Cola: El sistema exhibe una alta tasa de recuperación. Una vez restablecidas las condiciones operativas normales, la cola decreció rápidamente de 12 a 2 aeronaves en aproximadamente una hora. - Adecuación de la Tasa de Servicio: Este comportamiento indica que la tasa de servicio nominal (\\(\\mu\\) = 25 operaciones/hora) está adecuadamente dimensionada para absorber los retrasos acumulados, siempre y cuando las ventanas de interrupción climática no excedan una duración crítica.\n\n\nConclusión General:\nLa simulación validó la utilidad de la integración híbrida de los dos paradigmas de simulación estudiados. El tráfico aéreo siguió el perfil de horas de alta demanda (NHPP), pero fue severamente distorsionado por las inyecciones de caos del motor climático (Gillespie), generando escenarios de congestión realistas que un modelo estático de teoría de colas no habría podido evidenciar.\n\n\nUso de Inteligencia Artificial\nEn este trabajo se utilizó inteligencia artificial (Gemini) para:\n\nTransformar código base de R a Python\nFormatear gráficos\nFormatear código para que no ocupe más de 70 caracteres (restricción de renderizado de Quarto)\nRevisar estilo de la redacción\n\n\n\nAnexo I - Código base en Python\n\n# Código base\n# Linear Congruential Generator (LGC)\n\ndef lcg(n, seed=123, a=1103515245, c=12345, m=2**31):\n    x = seed\n    seq = []\n    for _ in range(n):\n        x = (a * x + c) % m\n        seq.append(x/m)\n    return seq\n\nalea = lcg(10)\nfor i in range(len(alea)):\n    print(alea[i])\n\n0.20531828328967094\n0.6873863865621388\n0.7767890496179461\n0.4024707484059036\n0.5324797946959734\n0.10148253152146935\n0.6351402262225747\n0.34936570515856147\n0.7226534709334373\n0.027218241710215807\n\n\n\n# Código base\n# Estimación de Pi por Monte Carlo\n\nimport numpy as np\n\nN = 100000\nu1 = np.random.uniform(0, 1, N)\nu2 = np.random.uniform(0, 1, N)\npi_est = 4 * np.mean(u1**2 + u2**2 &lt;= 1)\n\nprint(f\"Valor estimado de pi: {pi_est}\")\n\nValor estimado de pi: 3.14836\n\n\n\n# Código base\n# Implementación Metropolis–Hastings\n\nimport numpy as np\nfrom scipy.stats import binom\n\ndef mh(n_iter=5000, start=0.5, prop_sd=0.1):\n    # Inicializar la cadena (array lleno de ceros)\n    theta = np.zeros(n_iter)\n    theta[0] = start\n    \n    # Bucle desde el segundo elemento (índice 1) hasta el final\n    for t in range(1, n_iter):\n        # Propuesta: Distribución normal centrada en el theta anterior\n        # Equivalente en R: rnorm(1, theta[t-1], prop_sd)\n        prop = np.random.normal(loc=theta[t-1], scale=prop_sd)\n        \n        # Verificación de límites (Prior Uniforme Implícito [0,1])\n        # Si la propuesta se sale del rango [0, 1], se rechaza inmediatamente\n        if prop &lt; 0 or prop &gt; 1:\n            theta[t] = theta[t-1]\n        else:\n            # Calcular la Verosimilitud (Likelihood) usando la función \n            # de masa de probabilidad\n            # Equivalente en R: dbinom(7, 10, prob)\n            # * 1 representa el Prior (Uniforme)\n            num = binom.pmf(k=7, n=10, p=prop) * 1      \n            den = binom.pmf(k=7, n=10, p=theta[t-1]) * 1\n            \n            # Probabilidad de aceptación (alpha)\n            ratio = num / den\n            alpha = min(1, ratio)\n            \n            # Paso de Aceptación o Rechazo\n            # Generamos un número aleatorio uniforme \n            # y lo comparamos con alpha\n            if np.random.uniform() &lt; alpha:\n                # Aceptar la propuesta\n                theta[t] = prop     \n            else:\n                # Rechazar y mantener el valor anterior\n                theta[t] = theta[t-1] \n                \n    return theta\n\nchain = mh()\nprint(f\"Media Posterior Estimada: {np.mean(chain):.4f}\")\n\nMedia Posterior Estimada: 0.6778\n\n\n\n# Codigo base \n# Simulación de Eventos Discretos (M/M/1 o M/M/c)\n\nimport numpy as np\n\ndef simulate_mm1(lam, mu, T_max):\n    # Nota: 'lambda' es una palabra reservada en Python, usamos 'lam'\n    t = 0.0\n    n = 0\n    \n    # R usa la tasa (rate) para rexp: rexp(1, lambda)\n    # Numpy usa la escala (scale = 1/rate): exponential(1/lam)\n    t_arr = np.random.exponential(scale=1/lam)\n    t_dep = float('inf')  # Infinito en Python\n    \n    arrivals = 0\n    departures = 0\n    wait_times = []  \n    \n    while t &lt; T_max:\n        if t_arr &lt; t_dep:\n            # --- Evento de Llegada ---\n            t = t_arr\n            n += 1\n            arrivals += 1\n            \n            # Si el servidor estaba libre (n=1 tras la llegada)\n            # se programa servicio\n            if n == 1:\n                t_dep = t + np.random.exponential(scale=1/mu)\n            \n            # Programar la siguiente llegada\n            t_arr = t + np.random.exponential(scale=1/lam)\n            \n        else:\n            # Evento de Salida\n            t = t_dep\n            n -= 1\n            departures += 1\n            # Se agrega el tiempo a la lista\n            wait_times.append(t_dep) \n            \n            if n &gt; 0:\n                # Si quedan clientes, programar la siguiente salida\n                t_dep = t + np.random.exponential(scale=1/mu)\n            else:\n                # Si no hay nadie, la próxima salida es \"infinita\"\n                t_dep = float('inf')\n    \n    # Se devuelve un diccionario con los tiempos\n    return {\n        \"arrivals\": arrivals, \n        \"departures\": departures, \n        \"wait_times\": wait_times\n    }\n\n# Lambda = 2 clientes/min, Mu = 3 clientes/min, Tiempo = 100 min\nres = simulate_mm1(lam=2, mu=3, T_max=100)\n\nprint(f\"Llegadas totales: {res['arrivals']}\")\nprint(f\"Salidas totales: {res['departures']}\")\nprint(\"Últimos 5 tiempos de salida:\")\n\nfor i in range(len(res['wait_times'])-5,len(res['wait_times'])):\n    print(res['wait_times'][i])\n\nLlegadas totales: 215\nSalidas totales: 215\nÚltimos 5 tiempos de salida:\n98.43347389754513\n98.50944958825472\n99.40263056754019\n99.71489988562641\n100.03483812961011\n\n\n\n# Codigo base\n# Implementación Gillespie SSA\n\nimport numpy as np\nimport pandas as pd\n\ndef gillespie(Tmax, k1=10, k2=1):\n    t = 0.0\n    X = 0\n    \n    # Listas para almacenar el historial\n    ts = [t]\n    xs = [X]\n    \n    while t &lt; Tmax:\n        # Cálculo de propensiones\n        a1 = k1\n        a2 = k2 * X\n        a0 = a1 + a2\n        \n        # Si la propensión total es 0, el sistema se detiene\n        if a0 == 0:\n            break\n            \n        # 1. Generar tiempo hasta el próximo evento (tau)\n        # R: -log(runif(1)) / a0 \n        # Esto es matemáticamente generar una variable \n        # aleatoria Exponencial con tasa a0\n        tau = np.random.exponential(scale=1/a0)\n        \n        # 2. Determinar qué evento ocurre\n        r2 = np.random.uniform()\n        \n        if r2 * a0 &lt; a1:\n            # Evento 1: Nacimiento / Producción\n            X += 1\n        else:\n            # Evento 2: Muerte / Degradación\n            X = max(0, X - 1)\n            \n        # Actualizar tiempo y almacenar estado\n        t += tau\n        ts.append(t)\n        xs.append(X)\n        \n    # Devolvemos un DataFrame de Pandas \n    return pd.DataFrame({'time': ts, 'X': xs})\n\ndf_result = gillespie(Tmax=50)\n\n# Mostrar las primeras 5 filas\nprint(df_result.head())\n\n       time  X\n0  0.000000  0\n1  0.031461  1\n2  0.052792  2\n3  0.214520  3\n4  0.226669  4"
  }
]